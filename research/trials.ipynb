{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "316a4b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a91cdcc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\shyam\\\\OneDrive\\\\Documents\\\\GitHub\\\\AI-Bot\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a818a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d6576a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\shyam\\\\OneDrive\\\\Documents\\\\GitHub\\\\AI-Bot'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fbc717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c520694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data From the PDF File\n",
    "def load_pdf_file(data):\n",
    "    loader= DirectoryLoader(data,\n",
    "                            glob=\"*.pdf\",\n",
    "                            loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents=loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c5bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf_file(data='Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e6c1fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ad4dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7af27f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 37362\n"
     ]
    }
   ],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7c86ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e86ec194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip uninstall sentence-transformers huggingface-hub -y\n",
    "#%pip install sentence-transformers huggingface-hub --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a89bcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U langchain-huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74165d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d69950e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: C:/Users/shyam/models/all-MiniLM-L6-V2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shyam\\OneDrive\\Documents\\GitHub\\AI-Bot\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "def download_hugging_face_embeddings():\n",
    "    local_model_path = \"C:/Users/shyam/models/all-MiniLM-L6-V2\"\n",
    "    \n",
    "    if not os.path.exists(local_model_path):\n",
    "        raise FileNotFoundError(f\"Model path does not exist: {local_model_path}\")\n",
    "    \n",
    "    print(f\"Loading model from: {local_model_path}\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=local_model_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_hugging_face_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a755678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaacbc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "073880d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86fa4795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Pinecone library\n",
    "from pinecone import Pinecone\n",
    "import os\n",
    "\n",
    "# Initialize a Pinecone client with your API key\n",
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8635b079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"medicalbot\",\n",
       "    \"metric\": \"cosine\",\n",
       "    \"host\": \"medicalbot-hbpacab.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"dense\",\n",
       "    \"dimension\": 384,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os\n",
    "\n",
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"medicalbot\"\n",
    "\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384, \n",
    "    metric=\"cosine\", \n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\", \n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f81b17d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdaf599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f61ee4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x296843a2200>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62fb2bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf6a7bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is Acne?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9eebdfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c9854819-8252-4f54-b7dd-e1e59a9a0808', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 39.0, 'page_label': '40', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Gale Encyclopedia of Medicine Vol. 1 (A-B).pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       " Document(id='3162659e-9aec-481c-ae72-36fcc3dcee25', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 38.0, 'page_label': '39', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Gale Encyclopedia of Medicine Vol. 1 (A-B).pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 2 25\\nAcne\\nAcne vulgaris affecting a womanâ€™s face. Acne is the general\\nname given to a skin disorder in which the sebaceous\\nglands become inflamed.(Photograph by Biophoto Associ-\\nates, Photo Researchers, Inc. Reproduced by permission.)\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 25'),\n",
       " Document(id='12536e17-053e-48ba-a7e8-b9f439acf22e', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 37.0, 'page_label': '38', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Gale Encyclopedia of Medicine Vol. 1 (A-B).pdf', 'total_pages': 637.0}, page_content='Acidosis see Respiratory acidosis; Renal\\ntubular acidosis; Metabolic acidosis\\nAcne\\nDefinition\\nAcne is a common skin disease characterized by\\npimples on the face, chest, and back. It occurs when the\\npores of the skin become clogged with oil, dead skin\\ncells, and bacteria.\\nDescription\\nAcne vulgaris, the medical term for common acne, is\\nthe most common skin disease. It affects nearly 17 million\\npeople in the United States. While acne can arise at any')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "35115ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "GROQ_API_KEY=os.environ.get('GROQ_API_KEY')\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f936ffdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are revolutionizing the field of natural language processing (NLP) and have numerous applications in various industries. The importance of fast language models can be attributed to the following factors:\n",
      "\n",
      "1. **Speed and Efficiency**: Fast language models can process large amounts of text data at incredible speeds, making them ideal for applications where response time is crucial, such as chatbots, virtual assistants, and real-time sentiment analysis.\n",
      "2. **Improved User Experience**: Fast language models enable developers to create responsive and interactive applications that can engage users in real-time, leading to enhanced user experiences and increased satisfaction.\n",
      "3. **Scalability**: Fast language models can handle a massive volume of requests, making them perfect for large-scale applications, such as language translation, text summarization, and sentiment analysis, where speed and efficiency are essential.\n",
      "4. **Real-time Applications**: Fast language models are essential for real-time applications, such as:\n",
      "\t* Live chat support and customer service.\n",
      "\t* Sentiment analysis and opinion mining.\n",
      "\t* Text classification and entity recognition.\n",
      "\t* Language translation and localization.\n",
      "5. **Competitive Advantage**: Organizations that adopt fast language models can gain a competitive advantage by:\n",
      "\t* Responding quickly to customer inquiries and concerns.\n",
      "\t* Providing timely and accurate information to users.\n",
      "\t* Enhancing their brand reputation through efficient and effective communication.\n",
      "6. **Research and Development**: Fast language models facilitate researchers to experiment with new ideas, test hypotheses, and develop innovative applications, accelerating the pace of progress in NLP and related fields.\n",
      "7. **Cost Savings**: Fast language models can help organizations reduce costs associated with:\n",
      "\t* Human annotation and manual data processing.\n",
      "\t* Computational resources and infrastructure.\n",
      "\t* Development and deployment of NLP applications.\n",
      "8. **Enhanced Analytics**: Fast language models enable organizations to analyze large amounts of text data in real-time, providing valuable insights into customer behavior, preferences, and trends.\n",
      "9. **Security and Compliance**: Fast language models can help organizations detect and respond to security threats, such as phishing attacks and data breaches, in a timely and effective manner.\n",
      "10. **Accessibility**: Fast language models can facilitate the development of accessible applications, such as language translation and text-to-speech systems, which can improve the lives of people with disabilities and language barriers.\n",
      "\n",
      "In summary, fast language models are essential for a wide range of applications, from chatbots and virtual assistants to sentiment analysis and text summarization. Their importance lies in their ability to process large amounts of text data quickly and efficiently, providing organizations with a competitive advantage, cost savings, and enhanced analytics, while also improving user experiences and accessibility.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4e88e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.7)\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a95020e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import request\n",
    "def chat():\n",
    "    msg = request.form[\"msg\"]\n",
    "    input = msg\n",
    "    print(input)\n",
    "    response = chain.invoke({\"input\": msg, \"context\": \"\"})\n",
    "    response_terms = set(response.lower().split())\n",
    "    retriever_terms = set(retriever.keys())\n",
    "    if not response_terms.issubset(retriever_terms):\n",
    "        response = \"Yeh Meri Expertise Nahi Hai, so I Am Out.\"\n",
    "    print(\"Response : \", response)\n",
    "    return str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "db6ccf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Acromegaly and gigantism are rare hormonal disorders caused by an excess of growth hormone (GH) in the body. Gigantism occurs when the excess GH is produced before the bones have fully grown, typically during childhood, leading to excessive growth and tall stature. Acromegaly, on the other hand, occurs when the excess GH is produced after normal growth has stopped, typically in adulthood, leading to enlarged hands, feet, and facial features.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 94, 'prompt_tokens': 95, 'total_tokens': 189, 'completion_time': 0.341818182, 'prompt_time': 0.005043739, 'queue_time': 0.058719501, 'total_time': 0.346861921}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9a8b91ba77', 'finish_reason': 'stop', 'logprobs': None} id='run-564c9eb9-5899-452e-94c2-1b8bd93e8228-0' usage_metadata={'input_tokens': 95, 'output_tokens': 94, 'total_tokens': 189}\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"input\": \"what is Acromegaly and gigantism?\", \"context\": \"\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fe2a74dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Answer the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2a9740ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Stars are massive, luminous balls of gas that are held together by their own gravity. They are primarily composed of hydrogen and helium, and are the source of light and heat for many planets, including Earth. The Sun is an example of a star, and there are billions of other stars in the universe.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 88, 'total_tokens': 151, 'completion_time': 0.229090909, 'prompt_time': 0.005905042, 'queue_time': 0.060029678, 'total_time': 0.234995951}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9a8b91ba77', 'finish_reason': 'stop', 'logprobs': None} id='run-81e05692-c060-4f3f-a421-96f414c4ee9c-0' usage_metadata={'input_tokens': 88, 'output_tokens': 63, 'total_tokens': 151}\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"input\": \"What is stars?\", \"context\": \"\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
